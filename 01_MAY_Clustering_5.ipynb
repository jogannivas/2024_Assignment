{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1:-\n",
        "\n",
        "A contingency matrix, also known as a confusion matrix, is a table that is often used to evaluate the performance of a classification model. It provides a summary of the predicted and actual classifications of a dataset. The matrix compares the predicted classes (or labels) with the true classes and breaks down the results into four categories: true positive (TP), true negative (TN), false positive (FP), and false negative (FN). These categories are defined as follows:\n",
        "\n",
        "True Positive (TP): The instances where the model correctly predicted the positive class.\n",
        "\n",
        "True Negative (TN): The instances where the model correctly predicted the negative class.\n",
        "\n",
        "False Positive (FP): The instances where the model incorrectly predicted the positive class when the true class was negative (Type I error).\n",
        "\n",
        "False Negative (FN): The instances where the model incorrectly predicted the negative class when the true class was positive (Type II error)."
      ],
      "metadata": {
        "id": "yLYN-lmk6iKC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2:-\n",
        "\n",
        "A pair confusion matrix is an extension of the traditional confusion matrix that is specifically designed for binary classification problems where there are pairs of classes that are of particular interest. In standard binary classification, there are two classes: positive and negative. However, in certain situations, it might be more relevant to focus on pairs of classes, especially when dealing with imbalanced datasets or when there is a specific interest in the relationship between two particular classes.\n",
        "\n",
        "Here's how a pair confusion matrix differs from a regular confusion matrix:\n",
        "\n",
        "Classes of Interest: In a regular confusion matrix, you have two classes: positive and negative. In a pair confusion matrix, you focus on a specific pair of classes, often denoting one as the \"target\" class and the other as the \"non-target\" class. This allows for a more detailed analysis of the performance regarding those specific classes.\n",
        "\n",
        "Matrix Structure: The pair confusion matrix has a structure similar to the regular confusion matrix, but it specifically highlights the TP, TN, FP, and FN for the chosen\n",
        "Metrics Calculation: Similar performance metrics such as precision, recall, specificity, and accuracy can be calculated using the values in the pair confusion matrix, focusing on the chosen pair of classes.\n",
        "\n",
        "The pair confusion matrix can be useful in situations where the distinction between two specific classes is critical or when dealing with imbalanced datasets where one class is more important than the other. For example, in medical diagnostics, where the focus is often on detecting a specific condition, a pair confusion matrix might be used to evaluate the model's performance in correctly identifying instances of that condition (TP) and avoiding false alarms (FP)"
      ],
      "metadata": {
        "id": "0I0XXyLr6iNf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3:-\n",
        "\n",
        "Extrinsic evaluation is the best way to evaluate the performance of a language model by embedding it in an application and measuring how much the application improves. It is an end-to-end evaluation where we can understand if a particular improvement in a component is really going to help the task at hand."
      ],
      "metadata": {
        "id": "aiQJel7u6iQT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4:-\n",
        "\n",
        "n an intrinsic evaluation, quality of NLP systems outputs is evaluated against pre-determined ground truth (reference text) whereas an extrinsic evaluation is aimed at evaluating systems outputs based on their impact on the performance of other NLP systems"
      ],
      "metadata": {
        "id": "d9mioi-m_JB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5:-\n",
        "\n",
        "The confusion matrix is a tool used to evaluate the performance of a model and is visually represented as a table. It provides a deeper layer of insight to data practitioners on the model's performance, errors, and weaknesses."
      ],
      "metadata": {
        "id": "H7a6HZHw_JKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6:-\n",
        "\n",
        "Evaluating the performance of unsupervised learning algorithms can be challenging since there are no explicit target labels to compare predictions against. Instead, intrinsic evaluation measures are used, which assess the quality of the model based on its internal characteristics. Here are some common intrinsic measures used for unsupervised learning:\n",
        "\n",
        "Silhouette Score:\n",
        "\n",
        "The silhouette score measures how similar an object is to its own cluster compared to other clusters. It ranges from -1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. An overall high silhouette score suggests a well-defined clustering.\n",
        "Davies-Bouldin Index:\n",
        "\n",
        "The Davies-Bouldin index measures the compactness and separation between clusters. A lower Davies-Bouldin index indicates better clustering, with small values indicating dense, well-separated clusters.\n",
        "Calinski-Harabasz Index (Variance Ratio Criterion):\n",
        "\n",
        "This index evaluates the ratio of the between-cluster variance to within-cluster variance. A higher Calinski-Harabasz index indicates better-defined clusters.\n",
        "Dunn Index:\n",
        "\n",
        "The Dunn index assesses the compactness and separation between clusters. It is calculated as the ratio of the minimum inter-cluster distance to the maximum intra-cluster distance. Higher values are generally preferable, indicating well-separated clusters.\n",
        "Inertia (Within-Cluster Sum of Squares):\n",
        "\n",
        "Inertia measures the sum of squared distances between each data point and the centroid of its assigned cluster. Lower inertia suggests more compact clusters. It is commonly used in k-means clustering.\n",
        "Gap Statistics:\n",
        "\n",
        "Gap statistics compare the goodness of the clustering in the dataset to what would be expected by random chance. A larger gap statistic suggests a better clustering result.\n",
        "Interpretation of these measures:\n",
        "\n",
        "Higher is Better: Silhouette score, Calinski-Harabasz index, and Dunn index. In these cases, a higher value indicates a better clustering result.\n",
        "\n",
        "Lower is Better: Davies-Bouldin index and Inertia. Lower values suggest better-defined and more compact clusters.\n",
        "\n",
        "Comparison to Random Chance: Gap statistics. The goal is to have a gap statistic significantly larger than what would be expected by random chance."
      ],
      "metadata": {
        "id": "s2lkL1iS_mIG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7:-\n",
        "\n",
        "Accuracy can be misleading or insufficient when the data set is imbalanced or has more than two classes. For example, if 90 out of 100 instances are positive and your model predicts all as positive, the accuracy is 0.9 or 90%. However, this is not a good model because it ignores any negative instances."
      ],
      "metadata": {
        "id": "65-ARRS0_1fg"
      }
    }
  ]
}